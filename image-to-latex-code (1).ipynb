{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:00:35.334205Z","iopub.execute_input":"2025-07-29T23:00:35.334493Z","iopub.status.idle":"2025-07-29T23:00:35.615663Z","shell.execute_reply.started":"2025-07-29T23:00:35.334469Z","shell.execute_reply":"2025-07-29T23:00:35.614856Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n%pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n%pip install sentencepiece protobuf datasets huggingface_hub hf_transfer nltk python-Levenshtein\n%pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:00:37.341770Z","iopub.execute_input":"2025-07-29T23:00:37.342395Z","iopub.status.idle":"2025-07-29T23:00:54.290613Z","shell.execute_reply.started":"2025-07-29T23:00:37.342370Z","shell.execute_reply":"2025-07-29T23:00:54.289679Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHUGGINGFACE_TOKEN = user_secrets.get_secret(\"nlp\")\n\n!huggingface-cli login --token $HUGGINGFACE_TOKEN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:00:54.291856Z","iopub.execute_input":"2025-07-29T23:00:54.292092Z","iopub.status.idle":"2025-07-29T23:00:55.208943Z","shell.execute_reply.started":"2025-07-29T23:00:54.292071Z","shell.execute_reply":"2025-07-29T23:00:55.208074Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `nlp` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `nlp`\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from unsloth import FastVisionModel\nimport torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import sentence_bleu\nimport Levenshtein\nimport subprocess\nimport tempfile\nimport os\nfrom transformers import TextStreamer\nfrom unsloth import is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig\nimport nltk\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:01:52.964028Z","iopub.execute_input":"2025-07-29T23:01:52.964295Z","iopub.status.idle":"2025-07-29T23:01:53.111351Z","shell.execute_reply.started":"2025-07-29T23:01:52.964276Z","shell.execute_reply":"2025-07-29T23:01:53.110580Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"fourbit_models = [\n    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",\n    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\"\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:02:12.448729Z","iopub.execute_input":"2025-07-29T23:02:12.449014Z","iopub.status.idle":"2025-07-29T23:02:12.452985Z","shell.execute_reply.started":"2025-07-29T23:02:12.448994Z","shell.execute_reply":"2025-07-29T23:02:12.452248Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model, tokenizer = FastVisionModel.from_pretrained(\n    \"unsloth/Qwen2-VL-7B-Instruct\",\n    load_in_4bit=True,\n    use_gradient_checkpointing=\"unsloth\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:02:21.872088Z","iopub.execute_input":"2025-07-29T23:02:21.872829Z","iopub.status.idle":"2025-07-29T23:03:53.503096Z","shell.execute_reply.started":"2025-07-29T23:02:21.872792Z","shell.execute_reply":"2025-07-29T23:03:53.502158Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.7.11: Fast Qwen2 patching. Transformers: 4.52.4.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdfb11b295b94181b9f5f7cf1a26a9c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8e5970c6e7948b69885b15df99e211f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/572 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03dfcbbfd184704ae67a995cc605234"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b2917acc44484b816628b63773ba5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a34a6a08236b43c3b607448b685ef80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02076518f30b41e8b9ac74fc80f2591e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0691d0e7db0e4562a6c406dba590196a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e626c651f1504408a2a7f672a1dc5a66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6303276981b4c68810ece8e8c8098b6"}},"metadata":{}},{"name":"stderr","text":"You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"chat_template.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c313706519fe41efa37f27c73cc84af0"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    finetune_vision_layers=True,\n    finetune_language_layers=True,\n    finetune_attention_modules=True,\n    finetune_mlp_modules=True,\n\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    random_state = 3407,\n    use_rslora=False,\n    loftq_config=None\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:03:53.504373Z","iopub.execute_input":"2025-07-29T23:03:53.504688Z","iopub.status.idle":"2025-07-29T23:03:59.626149Z","shell.execute_reply.started":"2025-07-29T23:03:53.504662Z","shell.execute_reply":"2025-07-29T23:03:59.625403Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model.visual` require gradients\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"unsloth/Latex_OCR\", split=\"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:04:44.924295Z","iopub.execute_input":"2025-07-29T23:04:44.924921Z","iopub.status.idle":"2025-07-29T23:04:57.635474Z","shell.execute_reply.started":"2025-07-29T23:04:44.924886Z","shell.execute_reply":"2025-07-29T23:04:57.634764Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/344M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"101fa332afde4fedb1bf9bc537e09495"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/38.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e98096623db4fb7869f3b4dcfffb0cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/68686 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd1d57f7fcda496faf39ce34a88e15e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7632 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec1ab42c4ad49e887419902ab6fc420"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"split_data = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_data[\"train\"]\ntemp_dataset = split_data[\"test\"]\n\n# Split temp into eval (10%) and test (10%)\neval_test_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\neval_dataset = eval_test_split[\"train\"]\ntest_dataset = eval_test_split[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:01.742282Z","iopub.execute_input":"2025-07-29T23:05:01.742608Z","iopub.status.idle":"2025-07-29T23:05:01.790574Z","shell.execute_reply.started":"2025-07-29T23:05:01.742560Z","shell.execute_reply":"2025-07-29T23:05:01.789794Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"print(f\"Train samples: {len(train_dataset)}\")\nprint(f\"Eval samples: {len(eval_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:03.524467Z","iopub.execute_input":"2025-07-29T23:05:03.525209Z","iopub.status.idle":"2025-07-29T23:05:03.530954Z","shell.execute_reply.started":"2025-07-29T23:05:03.525171Z","shell.execute_reply":"2025-07-29T23:05:03.530048Z"}},"outputs":[{"name":"stdout","text":"Train samples: 54948\nEval samples: 6869\nTest samples: 6869\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:05.402484Z","iopub.execute_input":"2025-07-29T23:05:05.403337Z","iopub.status.idle":"2025-07-29T23:05:05.408428Z","shell.execute_reply.started":"2025-07-29T23:05:05.403306Z","shell.execute_reply":"2025-07-29T23:05:05.407574Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['image', 'text'],\n    num_rows: 54948\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"train_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:07.628641Z","iopub.execute_input":"2025-07-29T23:05:07.629565Z","iopub.status.idle":"2025-07-29T23:05:07.679546Z","shell.execute_reply.started":"2025-07-29T23:05:07.629527Z","shell.execute_reply":"2025-07-29T23:05:07.678860Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=200x40>,\n 'text': 'X _ { M } ( m ) = { \\\\frac { d } { d t } } ( \\\\exp ( t X ) \\\\cdot m ) \\\\Big | _ { t = 0 } .'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"train_dataset[0][\"image\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:10.767825Z","iopub.execute_input":"2025-07-29T23:05:10.768107Z","iopub.status.idle":"2025-07-29T23:05:10.779147Z","shell.execute_reply.started":"2025-07-29T23:05:10.768085Z","shell.execute_reply":"2025-07-29T23:05:10.778258Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<PIL.PngImagePlugin.PngImageFile image mode=RGB size=200x40>","image/png":"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","image/jpeg":"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"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"train_dataset[1][\"image\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:13.441689Z","iopub.execute_input":"2025-07-29T23:05:13.442005Z","iopub.status.idle":"2025-07-29T23:05:13.448938Z","shell.execute_reply.started":"2025-07-29T23:05:13.441981Z","shell.execute_reply":"2025-07-29T23:05:13.448222Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>","image/png":"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","image/jpeg":"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"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"train_dataset[1][\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:15.519166Z","iopub.execute_input":"2025-07-29T23:05:15.520033Z","iopub.status.idle":"2025-07-29T23:05:15.526841Z","shell.execute_reply.started":"2025-07-29T23:05:15.519994Z","shell.execute_reply":"2025-07-29T23:05:15.526033Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'\\\\left[ { { \\\\cal H } _ { \\\\mathrm { i n t } } [ x ^ { \\\\prime } ] , { \\\\cal H } _ { \\\\mathrm { i n t } } [ x ^ { \\\\prime \\\\prime } ] } \\\\right] _ { c } = 0 .'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"def exact_match_accuracy(predictions, targets):\n    \"\"\"Calculate percentage of exact matches between predictions and targets\"\"\"\n    matches = [pred.strip() == target.strip() for pred, target in zip(predictions, targets)]\n    return np.mean(matches)\n\ndef calculate_bleu_scores(predictions, targets):\n    \"\"\"Calculate BLEU scores for each prediction-target pair\"\"\"\n    bleu_scores = []\n    for pred, target in zip(predictions, targets):\n        pred_tokens = pred.strip().split()\n        target_tokens = target.strip().split()\n        if len(target_tokens) == 0:\n            bleu_scores.append(0.0)\n        else:\n            try:\n                score = sentence_bleu([target_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n                bleu_scores.append(score)\n            except:\n                bleu_scores.append(0.0)\n    return bleu_scores\n\ndef calculate_edit_distances(predictions, targets):\n    \"\"\"Calculate edit distances between predictions and targets\"\"\"\n    distances = []\n    for pred, target in zip(predictions, targets):\n        distance = Levenshtein.distance(pred.strip(), target.strip())\n        distances.append(distance)\n    return distances","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:17.474008Z","iopub.execute_input":"2025-07-29T23:05:17.474298Z","iopub.status.idle":"2025-07-29T23:05:17.481251Z","shell.execute_reply.started":"2025-07-29T23:05:17.474278Z","shell.execute_reply":"2025-07-29T23:05:17.480367Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def check_latex_compilation(latex_code):\n    \"\"\"Check if LaTeX code compiles successfully\"\"\"\n    latex_document = f\"\"\"\n    \\\\documentclass{{article}}\n    \\\\usepackage{{amsmath, amssymb, amsfonts}}\n    \\\\begin{{document}}\n    $${latex_code}$$\n    \\\\end{{document}}\n    \"\"\"\n    \n    try:\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.tex', delete=False) as f:\n            f.write(latex_document)\n            tex_file = f.name\n        \n        result = subprocess.run(['pdflatex', '-interaction=nonstopmode', tex_file], \n                              capture_output=True, timeout=10, cwd='/tmp')\n        \n        # Clean up temporary files\n        base_name = tex_file[:-4]\n        for ext in ['.tex', '.pdf', '.log', '.aux']:\n            try:\n                os.unlink(base_name + ext)\n            except:\n                pass\n                \n        return result.returncode == 0\n    except Exception as e:\n        return False\n\ndef compilation_success_rate(predictions):\n    \"\"\"Calculate percentage of predictions that compile successfully\"\"\"\n    success_count = sum(check_latex_compilation(pred) for pred in predictions)\n    return success_count / len(predictions) if len(predictions) > 0 else 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:19.991285Z","iopub.execute_input":"2025-07-29T23:05:19.991548Z","iopub.status.idle":"2025-07-29T23:05:19.997834Z","shell.execute_reply.started":"2025-07-29T23:05:19.991529Z","shell.execute_reply":"2025-07-29T23:05:19.996837Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def evaluate_model(model, tokenizer, eval_dataset, max_samples=None, batch_size=1):\n    \"\"\"Evaluate the model on the evaluation dataset\"\"\"\n    model.eval()\n    predictions = []\n    targets = []\n    \n    # Limit evaluation samples if specified\n    samples_to_eval = eval_dataset if max_samples is None else eval_dataset.select(range(min(max_samples, len(eval_dataset))))\n    \n    instruction = \"Write the LaTeX representation for this image.\"\n    \n    print(f\"Evaluating on {len(samples_to_eval)} samples...\")\n    \n    for i, sample in enumerate(samples_to_eval):\n        if i % 10 == 0:\n            print(f\"Processing sample {i+1}/{len(samples_to_eval)}\")\n            \n        # Prepare input\n        messages = [\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]}\n        ]\n        \n        input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n        inputs = tokenizer(\n            sample[\"image\"], input_text,\n            add_special_tokens=False,\n            return_tensors=\"pt\",\n        ).to(\"cuda\")\n        \n        # Generate prediction\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs, \n                max_new_tokens=128, \n                use_cache=True, \n                temperature=0.1,  # Low temperature for consistent evaluation\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id\n            )\n        \n        # Decode prediction\n        prediction = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n        prediction = prediction.strip()\n        \n        predictions.append(prediction)\n        targets.append(sample[\"text\"])\n    \n    # Calculate metrics\n    print(\"Calculating metrics...\")\n    exact_match = exact_match_accuracy(predictions, targets)\n    bleu_scores = calculate_bleu_scores(predictions, targets)\n    edit_distances = calculate_edit_distances(predictions, targets)\n    \n    print(\"Checking LaTeX compilation (this may take a while)...\")\n    compilation_rate = compilation_success_rate(predictions)\n    \n    metrics = {\n        \"exact_match_accuracy\": exact_match,\n        \"average_bleu\": np.mean(bleu_scores),\n        \"median_bleu\": np.median(bleu_scores),\n        \"std_bleu\": np.std(bleu_scores),\n        \"average_edit_distance\": np.mean(edit_distances),\n        \"median_edit_distance\": np.median(edit_distances),\n        \"std_edit_distance\": np.std(edit_distances),\n        \"compilation_success_rate\": compilation_rate,\n        \"num_samples\": len(predictions)\n    }\n    \n    return metrics, predictions, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:24.051837Z","iopub.execute_input":"2025-07-29T23:05:24.052114Z","iopub.status.idle":"2025-07-29T23:05:24.061595Z","shell.execute_reply.started":"2025-07-29T23:05:24.052090Z","shell.execute_reply":"2025-07-29T23:05:24.060652Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def analyze_errors(predictions, targets, top_k=10):\n    \"\"\"Analyze common error patterns\"\"\"\n    errors = []\n    for pred, target in zip(predictions, targets):\n        if pred.strip() != target.strip():\n            errors.append({\n                'prediction': pred,\n                'target': target,\n                'edit_distance': Levenshtein.distance(pred, target)\n            })\n    \n    # Sort by edit distance\n    errors.sort(key=lambda x: x['edit_distance'], reverse=True)\n    \n    print(f\"\\nTop {top_k} most different predictions:\")\n    for i, error in enumerate(errors[:top_k]):\n        print(f\"{i+1}. Edit distance: {error['edit_distance']}\")\n        print(f\"   Target: {error['target']}\")\n        print(f\"   Prediction: {error['prediction']}\\n\")\n    \n    return errors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:50.676546Z","iopub.execute_input":"2025-07-29T23:05:50.677514Z","iopub.status.idle":"2025-07-29T23:05:50.683244Z","shell.execute_reply.started":"2025-07-29T23:05:50.677490Z","shell.execute_reply":"2025-07-29T23:05:50.682295Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"instruction = \"Write the LaTex representation for this image.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:53.863419Z","iopub.execute_input":"2025-07-29T23:05:53.864124Z","iopub.status.idle":"2025-07-29T23:05:53.868332Z","shell.execute_reply.started":"2025-07-29T23:05:53.864090Z","shell.execute_reply":"2025-07-29T23:05:53.867241Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def convert_to_conversation(sample):\n  conversation = [\n      {\"role\": \"user\",\n       \"content\": [\n           {\"type\": \"text\", \"text\": instruction},\n           {\"type\": \"image\", \"image\": sample[\"image\"]}\n       ]\n       },\n      {\"role\": \"assistant\",\n       \"content\": [\n           {\"type\": \"text\", \"text\": sample[\"text\"]}\n       ]\n       }\n  ]\n  return {\"messages\": conversation}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:55.960511Z","iopub.execute_input":"2025-07-29T23:05:55.961170Z","iopub.status.idle":"2025-07-29T23:05:55.966187Z","shell.execute_reply.started":"2025-07-29T23:05:55.961145Z","shell.execute_reply":"2025-07-29T23:05:55.965071Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"converted_train_dataset = [convert_to_conversation(sample) for sample in train_dataset]\nconverted_eval_dataset = [convert_to_conversation(sample) for sample in eval_dataset]\n\nprint(\"Dataset conversion completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:05:58.522208Z","iopub.execute_input":"2025-07-29T23:05:58.522479Z","iopub.status.idle":"2025-07-29T23:06:23.893673Z","shell.execute_reply.started":"2025-07-29T23:05:58.522459Z","shell.execute_reply":"2025-07-29T23:06:23.892807Z"}},"outputs":[{"name":"stdout","text":"Dataset conversion completed\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"convert_to_conversation(train_dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:06:23.903657Z","iopub.execute_input":"2025-07-29T23:06:23.904479Z","iopub.status.idle":"2025-07-29T23:06:23.916398Z","shell.execute_reply.started":"2025-07-29T23:06:23.904457Z","shell.execute_reply":"2025-07-29T23:06:23.915499Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'role': 'user',\n   'content': [{'type': 'text',\n     'text': 'Write the LaTex representation for this image.'},\n    {'type': 'image',\n     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=200x40>}]},\n  {'role': 'assistant',\n   'content': [{'type': 'text',\n     'text': 'X _ { M } ( m ) = { \\\\frac { d } { d t } } ( \\\\exp ( t X ) \\\\cdot m ) \\\\Big | _ { t = 0 } .'}]}]}"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"# converted_dataset = [convert_to_conversation(sample) for sample in dataset]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:06:23.918439Z","iopub.execute_input":"2025-07-29T23:06:23.919019Z","iopub.status.idle":"2025-07-29T23:06:23.925754Z","shell.execute_reply.started":"2025-07-29T23:06:23.918992Z","shell.execute_reply":"2025-07-29T23:06:23.924852Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"converted_train_dataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:06:23.926697Z","iopub.execute_input":"2025-07-29T23:06:23.927070Z","iopub.status.idle":"2025-07-29T23:06:23.938373Z","shell.execute_reply.started":"2025-07-29T23:06:23.927044Z","shell.execute_reply":"2025-07-29T23:06:23.937505Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'role': 'user',\n   'content': [{'type': 'text',\n     'text': 'Write the LaTex representation for this image.'},\n    {'type': 'image',\n     'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>}]},\n  {'role': 'assistant',\n   'content': [{'type': 'text',\n     'text': '\\\\left[ { { \\\\cal H } _ { \\\\mathrm { i n t } } [ x ^ { \\\\prime } ] , { \\\\cal H } _ { \\\\mathrm { i n t } } [ x ^ { \\\\prime \\\\prime } ] } \\\\right] _ { c } = 0 .'}]}]}"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"FastVisionModel.for_inference(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:06:23.939129Z","iopub.execute_input":"2025-07-29T23:06:23.939366Z","iopub.status.idle":"2025-07-29T23:06:23.980986Z","shell.execute_reply.started":"2025-07-29T23:06:23.939345Z","shell.execute_reply":"2025-07-29T23:06:23.979210Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2VLForConditionalGeneration(\n      (model): Qwen2VLModel(\n        (visual): Qwen2VisionTransformerPretrainedModel(\n          (patch_embed): PatchEmbed(\n            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n          )\n          (rotary_pos_emb): VisionRotaryEmbedding()\n          (blocks): ModuleList(\n            (0-18): 19 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (19): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (20-21): 2 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (22): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (23-28): 6 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (29): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (30-31): 2 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n          )\n          (merger): PatchMerger(\n            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n            (mlp): Sequential(\n              (0): Linear(in_features=5120, out_features=5120, bias=True)\n              (1): GELU(approximate='none')\n              (2): Linear(in_features=5120, out_features=3584, bias=True)\n            )\n          )\n        )\n        (language_model): Qwen2VLTextModel(\n          (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n          (layers): ModuleList(\n            (0-27): 28 x Qwen2VLDecoderLayer(\n              (self_attn): Qwen2VLSdpaAttention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): Qwen2VLRotaryEmbedding()\n              )\n              (mlp): Qwen2MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=18944, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=18944, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=18944, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): SiLU()\n              )\n              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n            )\n          )\n          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n          (rotary_emb): Qwen2VLRotaryEmbedding()\n        )\n      )\n      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"# Baseline Evaluation (before training)\nprint(\"=== BASELINE EVALUATION (BEFORE TRAINING) ===\")\nFastVisionModel.for_inference(model)\nbaseline_metrics, baseline_preds, baseline_targets = evaluate_model(\n    model, tokenizer, eval_dataset, max_samples=50\n)\n\nprint(\"\\nBaseline Results:\")\nfor metric, value in baseline_metrics.items():\n    if isinstance(value, float):\n        print(f\"{metric}: {value:.4f}\")\n    else:\n        print(f\"{metric}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:06:23.981736Z","iopub.execute_input":"2025-07-29T23:06:23.981962Z","iopub.status.idle":"2025-07-29T23:15:44.265648Z","shell.execute_reply.started":"2025-07-29T23:06:23.981938Z","shell.execute_reply":"2025-07-29T23:15:44.264512Z"}},"outputs":[{"name":"stdout","text":"=== BASELINE EVALUATION (BEFORE TRAINING) ===\nEvaluating on 50 samples...\nProcessing sample 1/50\nProcessing sample 11/50\nProcessing sample 21/50\nProcessing sample 31/50\nProcessing sample 41/50\nCalculating metrics...\nChecking LaTeX compilation (this may take a while)...\n\nBaseline Results:\nexact_match_accuracy: 0.0000\naverage_bleu: 0.6589\nmedian_bleu: 0.7103\nstd_bleu: 0.2334\naverage_edit_distance: 40.1600\nmedian_edit_distance: 23.0000\nstd_edit_distance: 46.2573\ncompilation_success_rate: 0.0000\nnum_samples: 50\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"image = train_dataset[1][\"image\"]\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": instruction},\n            {\"type\": \"image\", \"image\": image}\n        ]\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:15:44.268896Z","iopub.execute_input":"2025-07-29T23:15:44.269186Z","iopub.status.idle":"2025-07-29T23:15:44.275474Z","shell.execute_reply.started":"2025-07-29T23:15:44.269162Z","shell.execute_reply":"2025-07-29T23:15:44.274745Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\ninputs = tokenizer(\n    image, input_text,\n    add_special_tokens = False,\n    return_tensors = \"pt\",\n).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:15:44.276283Z","iopub.execute_input":"2025-07-29T23:15:44.276535Z","iopub.status.idle":"2025-07-29T23:15:44.292467Z","shell.execute_reply.started":"2025-07-29T23:15:44.276513Z","shell.execute_reply":"2025-07-29T23:15:44.291749Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n_ = model.generate(**inputs, streamer= text_streamer, max_new_tokens = 128, use_cache=True, temperature=1.5, min_p=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:15:44.293662Z","iopub.execute_input":"2025-07-29T23:15:44.293934Z","iopub.status.idle":"2025-07-29T23:15:55.043418Z","shell.execute_reply.started":"2025-07-29T23:15:44.293912Z","shell.execute_reply":"2025-07-29T23:15:55.042629Z"}},"outputs":[{"name":"stdout","text":"$$\\left[ \\mathcal { H } _ { \\mathrm { i n } } ( x ^ { \\prime } ) , \\mathcal { H } _ { \\mathrm { i n } } ( x ^ { \\prime \\prime } ) \\right] _ { \\mathrm { c } } = 0 .$$<|im_end|>\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:15:55.044251Z","iopub.execute_input":"2025-07-29T23:15:55.044518Z","iopub.status.idle":"2025-07-29T23:15:55.050597Z","shell.execute_reply.started":"2025-07-29T23:15:55.044495Z","shell.execute_reply":"2025-07-29T23:15:55.049873Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<PIL.PngImagePlugin.PngImageFile image mode=RGB size=160x40>","image/png":"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","image/jpeg":"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"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"from unsloth import is_bf16_supported\nfrom unsloth.trainer import UnslothVisionDataCollator\nfrom trl import SFTTrainer, SFTConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:15:55.051537Z","iopub.execute_input":"2025-07-29T23:15:55.052234Z","iopub.status.idle":"2025-07-29T23:15:55.061052Z","shell.execute_reply.started":"2025-07-29T23:15:55.052199Z","shell.execute_reply":"2025-07-29T23:15:55.060214Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"FastVisionModel.for_inference(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:15:55.061919Z","iopub.execute_input":"2025-07-29T23:15:55.062573Z","iopub.status.idle":"2025-07-29T23:15:55.103317Z","shell.execute_reply.started":"2025-07-29T23:15:55.062551Z","shell.execute_reply":"2025-07-29T23:15:55.102493Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2VLForConditionalGeneration(\n      (model): Qwen2VLModel(\n        (visual): Qwen2VisionTransformerPretrainedModel(\n          (patch_embed): PatchEmbed(\n            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n          )\n          (rotary_pos_emb): VisionRotaryEmbedding()\n          (blocks): ModuleList(\n            (0-18): 19 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (19): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (20-21): 2 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (22): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (23-28): 6 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (29): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (30-31): 2 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n          )\n          (merger): PatchMerger(\n            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n            (mlp): Sequential(\n              (0): Linear(in_features=5120, out_features=5120, bias=True)\n              (1): GELU(approximate='none')\n              (2): Linear(in_features=5120, out_features=3584, bias=True)\n            )\n          )\n        )\n        (language_model): Qwen2VLTextModel(\n          (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n          (layers): ModuleList(\n            (0-27): 28 x Qwen2VLDecoderLayer(\n              (self_attn): Qwen2VLSdpaAttention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): Qwen2VLRotaryEmbedding()\n              )\n              (mlp): Qwen2MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=18944, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=18944, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=18944, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): SiLU()\n              )\n              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n            )\n          )\n          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n          (rotary_emb): Qwen2VLRotaryEmbedding()\n        )\n      )\n      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    max_seq_length=2048,\n    data_collator=UnslothVisionDataCollator(model, tokenizer),\n    train_dataset=converted_train_dataset,\n    eval_dataset=converted_eval_dataset[:100],  # Limit eval dataset for faster training\n    args=SFTConfig(\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=100,\n        learning_rate=2e-4,\n        fp16=not is_bf16_supported(),\n        bf16=is_bf16_supported(),\n        logging_steps=5,\n        eval_steps=25,\n        eval_strategy=\"steps\",\n        save_steps=50,\n        save_total_limit=2,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"none\",\n        remove_unused_columns=False,\n        dataset_text_field=\"\",\n        dataset_kwargs={\"skip_prepare_dataset\": True},\n        dataset_num_proc=4,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        dataloader_num_workers=0,\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:16:51.922221Z","iopub.execute_input":"2025-07-29T23:16:51.922508Z","iopub.status.idle":"2025-07-29T23:16:52.183884Z","shell.execute_reply.started":"2025-07-29T23:16:51.922486Z","shell.execute_reply":"2025-07-29T23:16:52.183132Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Model does not have a default image size - using 512\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:16:55.042250Z","iopub.execute_input":"2025-07-29T23:16:55.042969Z","iopub.status.idle":"2025-07-29T23:37:50.085714Z","shell.execute_reply.started":"2025-07-29T23:16:55.042936Z","shell.execute_reply":"2025-07-29T23:37:50.084789Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 54,948 | Num Epochs = 1 | Total steps = 100\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 50,855,936 of 8,342,231,552 (0.61% trained)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 19:54, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>0.129200</td>\n      <td>0.124244</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.102000</td>\n      <td>0.112220</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.125300</td>\n      <td>0.110495</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.113600</td>\n      <td>0.107174</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but Qwen2VLForConditionalGeneration does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=100, training_loss=0.1910795745253563, metrics={'train_runtime': 1249.3267, 'train_samples_per_second': 1.281, 'train_steps_per_second': 0.08, 'total_flos': 1.2502958568026112e+16, 'train_loss': 0.1910795745253563})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"print(\"\\n=== POST-TRAINING EVALUATION ===\")\nFastVisionModel.for_inference(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:37:50.087568Z","iopub.execute_input":"2025-07-29T23:37:50.088670Z","iopub.status.idle":"2025-07-29T23:37:50.132834Z","shell.execute_reply.started":"2025-07-29T23:37:50.088638Z","shell.execute_reply":"2025-07-29T23:37:50.131622Z"}},"outputs":[{"name":"stdout","text":"\n=== POST-TRAINING EVALUATION ===\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2VLForConditionalGeneration(\n      (model): Qwen2VLModel(\n        (visual): Qwen2VisionTransformerPretrainedModel(\n          (patch_embed): PatchEmbed(\n            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n          )\n          (rotary_pos_emb): VisionRotaryEmbedding()\n          (blocks): ModuleList(\n            (0-18): 19 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (19): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (20-21): 2 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (22): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (23-28): 6 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (29): Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n            (30-31): 2 x Qwen2VLVisionBlock(\n              (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n              (attn): VisionSdpaAttention(\n                (qkv): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3840, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (proj): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n              (mlp): VisionMlp(\n                (fc1): lora.Linear(\n                  (base_layer): Linear(in_features=1280, out_features=5120, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=1280, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=5120, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act): QuickGELUActivation()\n                (fc2): lora.Linear(\n                  (base_layer): Linear(in_features=5120, out_features=1280, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=5120, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=1280, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n              )\n            )\n          )\n          (merger): PatchMerger(\n            (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n            (mlp): Sequential(\n              (0): Linear(in_features=5120, out_features=5120, bias=True)\n              (1): GELU(approximate='none')\n              (2): Linear(in_features=5120, out_features=3584, bias=True)\n            )\n          )\n        )\n        (language_model): Qwen2VLTextModel(\n          (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n          (layers): ModuleList(\n            (0-27): 28 x Qwen2VLDecoderLayer(\n              (self_attn): Qwen2VLSdpaAttention(\n                (q_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (k_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (v_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=512, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (o_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (rotary_emb): Qwen2VLRotaryEmbedding()\n              )\n              (mlp): Qwen2MLP(\n                (gate_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=18944, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (up_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=3584, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=18944, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (down_proj): lora.Linear4bit(\n                  (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n                  (lora_dropout): ModuleDict(\n                    (default): Identity()\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=18944, out_features=16, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=16, out_features=3584, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                  (lora_magnitude_vector): ModuleDict()\n                )\n                (act_fn): SiLU()\n              )\n              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n            )\n          )\n          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n          (rotary_emb): Qwen2VLRotaryEmbedding()\n        )\n      )\n      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"print(\"Evaluating on validation set...\")\nval_metrics, val_predictions, val_targets = evaluate_model(\n    model, tokenizer, eval_dataset, max_samples=100\n)\n\nprint(\"\\nValidation Results:\")\nfor metric, value in val_metrics.items():\n    if isinstance(value, float):\n        print(f\"{metric}: {value:.4f}\")\n    else:\n        print(f\"{metric}: {value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-29T23:37:50.134014Z","iopub.execute_input":"2025-07-29T23:37:50.134306Z"}},"outputs":[{"name":"stdout","text":"Evaluating on validation set...\nEvaluating on 100 samples...\nProcessing sample 1/100\nProcessing sample 11/100\nProcessing sample 21/100\nProcessing sample 31/100\nProcessing sample 41/100\nProcessing sample 51/100\nProcessing sample 61/100\nProcessing sample 71/100\nProcessing sample 81/100\nProcessing sample 91/100\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"print(\"\\nEvaluating on test set...\")\ntest_metrics, test_predictions, test_targets = evaluate_model(\n    model, tokenizer, test_dataset, max_samples=100\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== FINAL TEST SET RESULTS ===\")\nfor metric, value in test_metrics.items():\n    if isinstance(value, float):\n        print(f\"{metric}: {value:.4f}\")\n    else:\n        print(f\"{metric}: {value}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n=== SAMPLE PREDICTIONS ===\")\nfor i in range(min(5, len(test_predictions))):\n    print(f\"\\nSample {i+1}:\")\n    print(f\"Target:     {test_targets[i]}\")\n    print(f\"Prediction: {test_predictions[i]}\")\n    match = \"✓\" if test_targets[i].strip() == test_predictions[i].strip() else \"✗\"\n    print(f\"Match: {match}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Error Analysis\nprint(\"\\n=== ERROR ANALYSIS ===\")\nerrors = analyze_errors(test_predictions, test_targets, top_k=5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Performance Comparison\nprint(\"\\n=== PERFORMANCE COMPARISON ===\")\nprint(\"Metric | Baseline | Post-Training | Improvement\")\nprint(\"-------|----------|---------------|------------\")\nfor metric in baseline_metrics.keys():\n    if isinstance(baseline_metrics[metric], float):\n        baseline_val = baseline_metrics[metric]\n        final_val = test_metrics[metric]\n        improvement = final_val - baseline_val\n        print(f\"{metric} | {baseline_val:.4f} | {final_val:.4f} | {improvement:+.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Interactive Testing Function\ndef test_single_image(model, tokenizer, image, show_image=True):\n    \"\"\"Test the model on a single image\"\"\"\n    FastVisionModel.for_inference(model)\n    \n    instruction = \"Write the LaTeX representation for this image.\"\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": instruction},\n            {\"type\": \"image\", \"image\": image}\n        ]}\n    ]\n    \n    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n    inputs = tokenizer(\n        image, input_text,\n        add_special_tokens=False,\n        return_tensors=\"pt\",\n    ).to(\"cuda\")\n    \n    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n    \n    print(\"Generated LaTeX:\")\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, \n            streamer=text_streamer, \n            max_new_tokens=128, \n            use_cache=True, \n            temperature=0.1, \n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    if show_image:\n        display(image)\n    \n    return outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test on a sample image\nprint(\"\\n=== TESTING ON SAMPLE IMAGE ===\")\nsample_image = test_dataset[0][\"image\"]\ntest_single_image(model, tokenizer, sample_image)\n\nprint(\"\\n=== TRAINING AND EVALUATION COMPLETED ===\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = dataset[2][\"image\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"instruction = \"Write the LaTeX representation for this image.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image\"},\n        {\"type\": \"text\", \"text\": instruction}\n    ]}\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\ninputs = tokenizer(\n    image,\n    input_text,\n    add_special_tokens=False,\n    return_tensors=\"pt\",\n).to(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt=True)\n\n_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}